{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/m0UWmqtJbXZLD38H2C8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Transformer_Implementation/blob/main/JAX%26PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing JAX and PyTorch"
      ],
      "metadata": {
        "id": "M5fHzpuBOXC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources: https://sjmielke.com/jax-purify.htm and https://kidger.site/thoughts/torch2jax/"
      ],
      "metadata": {
        "id": "AmnKeU5COcEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference in AutoDiff (reverse mode ofc) - PyTorch builds computational graph using Tensor, Modules and Parameter classes. As you go propogate forward, build computational graph by storing parent nodes, args and kwargs and functions. Wrap these forward functions in order to control gradients and correctly store this info. Then backward() on loss requires a store of forward and its corresponding \"backward\" function (derivative) to call upon during backprop. Topologically sort (depth first) for dp approach to chain rule and calculate derivatives in most memory efficient way possible (depth first gives that). JAX is more functional (think global parameters) and expresses gradient of functions (rather than actually computing the derivative) using the grad() transformation - this means that you get a function that you need to feed an input in order to get the gradient (rather than in PyTorch, where at a specific node, you get have grad_out - wrt to the loss - and working backwards gives you the derivative of the parent node wrt the current node, i.e. grad_out * grad of current function from input x to out)"
      ],
      "metadata": {
        "id": "KjlMdP48OmBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important things to look out for code-wise - PyTorch is oop with mutating states (think in-place operations like updating gradients in optimiser - we don't care about keeping original weight in memory, just want the updated one) - JAX relies on \"pure\" functions"
      ],
      "metadata": {
        "id": "TdRD6kHrQ-K8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For JAX (and fp in general) accurate summary - functions should always output the same values given same input. Also,there is no knowledge about whether the function has been called at all.\n"
      ],
      "metadata": {
        "id": "ItECVGUCW51k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def pure_fn_1(x):\n",
        "    return 2 * x\n",
        "\n",
        "def pure_fun_2(xs):\n",
        "    ys = []\n",
        "    for x in xs:\n",
        "      # Mutating stateful variables *inside* the function is fine!\n",
        "      ys.append(2 * x)\n",
        "    return ys\n",
        "\n",
        "def impure_fn_1(xs):\n",
        "    # Mutating arguments has lasting consequences outside the function! :(\n",
        "    xs.append(sum(xs))\n",
        "    return xs\n",
        "\n",
        "def impure_fn_2(x):\n",
        "    # Very obviously mutating global state is bad...\n",
        "    global num_execs\n",
        "    num_execs += 1\n",
        "    return 2 * x\n",
        "\n",
        "def impure_fn_3(x):\n",
        "    # ...but just accessing it is, too, because now the function depends on the\n",
        "    # execution context!\n",
        "    return num_execs * x\n",
        "\n",
        "def impure_fn_4(x):\n",
        "    # Things like IO are classic examples of impurity.\n",
        "    # All three of the following lines are violations of purity:\n",
        "    print(\"Hello!\")\n",
        "    user_input = input()\n",
        "    execution_time = time.time()\n",
        "    return 2 * x\n",
        "\n",
        "def impure_fn_5(x):\n",
        "    # Which constraint does this violate? Both, actually! You access the current\n",
        "    # state of randomness *and* advance the number generator!\n",
        "    p = random.random()\n",
        "    return p * x\n"
      ],
      "metadata": {
        "id": "QjcZYlYYXPd0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_execs = 0\n",
        "\n",
        "a = 3\n",
        "a_out = pure_fn_1(a)\n",
        "print(a_out) # 2 * 3\n",
        "\n",
        "b = [1,2,3]\n",
        "b_out = pure_fun_2(b)\n",
        "print(b_out) # essentially calling pure_fn_1 inside of function\n",
        "\n",
        "c = [1,2,3]\n",
        "c_out = impure_fn_1(c)\n",
        "print(c_out) # we have used an in place operation hence\n",
        "\n",
        "d = 3\n",
        "d_out = impure_fn_2(d)\n",
        "print(d) # used global keyword to access num_execs\n",
        "print(num_execs) # this is the global value that we've adjusted (defined a new variable and altered it)\n",
        "\n",
        "e = 3\n",
        "e_out = impure_fn_3(e)\n",
        "print(e) # altered as a result of the previous function being called first, and accessing the variable that has been changed\n",
        "\n",
        "f = 3\n",
        "f_out = impure_fn_4(f)\n",
        "print(f)\n",
        "\n",
        "g = 3\n",
        "g_out = impure_fn_5(g)\n",
        "print(g)# randomness violates consistency of output without fixed key\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7SUlCQIXr1L",
        "outputId": "422d62ae-fc9d-4bae-b433-4806a9a0ba55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[2, 4, 6]\n",
            "[1, 2, 3, 6]\n",
            "3\n",
            "1\n",
            "3\n",
            "Hello!\n",
            "hi\n",
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   JIT functions - use a single JIT decorator (or call) as all the operations are compiled within that function - make sure to you highest hierarchy function - extra/nested `jax.jit`s do nothing\n",
        "2.   Tracing (v. important) - for example, under a JIT function, can't use boolean/if statements as the whole graph is passed to the compiler at once and executed in full. For example, if you train a transformer and want to greedy decode during \"jitted\" training loop, can't include `if` statement for end token. Trace really means that the actual value is not known when code is being executed bu the object is known (array with same shape, dtype and paralleism approach) - changing the shape for example will trigger a recompile of the code. `if` can be used to define specific shape of computation graph before compilation time (i.e. if inside a model is fine as this is static, although of course cannot depend on contents of input array as this may change) . However, use jnp.where during computation - this allows you to use conditional statements - so for transformer decoding example, end token or masking/patch tokens will have to be generated to full voacb_size, but use jnp.where to choose output logits that aren't redundant. **Very** useful - inside of jits, use `jax.debug.print` (don't have to remove jit all the time!)\n",
        "3.  Jit has pytorch analogy in `torch.compile`\n",
        "4.  Pytrees - don't use generic python classes - use nn.Module from specific library\n",
        "5.  In place updates (or fa style) - use `array.at[].set()` rather than using indexing and changing values in numpy or pytorch. Problems occur if in backprop, function needs values of values not indexed - in this case the whole copy of array will be used. You will have to manually construct a new array to be certain i.e. set the values to whatever you want manually (so maybe use a for loop and jnp.concatenate)\n",
        "6.  JAX is about writing code that can be compiled intelligently. Compiler tries to speed up for loops by unfolding for loops. If recursively applying a function, it may create copies of function calls inside for loops which will have to be executed separately. To avoid this, write function calls explicitly, one after one anther, or use `jax.lax.scan` which eliminates need to call functions over again with carry (carry would be input in this case) and iterate over desired `range()`\n",
        "7.  `jax.lax.cond` is faster than `jnp.where` as it doesn't require evaluation of both inputs/branches - watch out when using this with vmap though (sometimes negates the speedup benefits)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "798auJqldksq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html"
      ],
      "metadata": {
        "id": "VRRosBEfkHBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr"
      ],
      "metadata": {
        "id": "XRPHw_aGkPyu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g=0.0\n",
        "def imp_global(x):\n",
        "  return x + g\n",
        "print(jax.jit(imp_global)(4.)) # 4 + 0 = 4\n",
        "g = 10. #change global value\n",
        "print(jax.jit(imp_global)(5.))  # expect 5 + 10 but get 5 + 0 - global value remains as 0.0 # using global keyword won't change anything\n",
        "print(jax.jit(imp_global)(jnp.array([4.]))) # however, changing the shape by using an array, triggers a recompile and thus updates the global value of variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVaKF1Aij_Fy",
        "outputId": "90ee138f-d6f6-48d3-f535-1005fa2f3d51"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "5.0\n",
            "[14.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.\n",
        "def imp_global_2(x):\n",
        "  global g\n",
        "  g = x\n",
        "  return x\n",
        "\n",
        "print(jax.jit(imp_global_2)(4.0))\n",
        "print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zSqc4HglaWm",
        "outputId": "6502c524-6ca6-4712-e428-6dcb2055b990"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n"
          ]
        }
      ]
    }
  ]
}