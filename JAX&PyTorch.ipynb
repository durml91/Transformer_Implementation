{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfqqH/isCZuRLemLU5ewZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/Transformer_Implementation/blob/main/JAX%26PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing JAX and PyTorch"
      ],
      "metadata": {
        "id": "M5fHzpuBOXC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources: https://sjmielke.com/jax-purify.htm and https://kidger.site/thoughts/torch2jax/"
      ],
      "metadata": {
        "id": "AmnKeU5COcEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference in AutoDiff (reverse mode ofc) - PyTorch builds computational graph using Tensor, Modules and Parameter classes. As you go propogate forward, build computational graph by storing parent nodes, args and kwargs and functions. Wrap these forward functions in order to control gradients and correctly store this info. Then backward() on loss requires a store of forward and its corresponding \"backward\" function (derivative) to call upon during backprop. Topologically sort (depth first) for dp approach to chain rule and calculate derivatives in most memory efficient way possible (depth first gives that). JAX is more functional (think global parameters) and expresses gradient of functions (rather than actually computing the derivative) using the grad() transformation - this means that you get a function that you need to feed an input in order to get the gradient (rather than in PyTorch, where at a specific node, you get have grad_out - wrt to the loss - and working backwards gives you the derivative of the parent node wrt the current node, i.e. grad_out * grad of current function from input x to out)"
      ],
      "metadata": {
        "id": "KjlMdP48OmBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoML1SnsNxWR"
      },
      "outputs": [],
      "source": []
    }
  ]
}